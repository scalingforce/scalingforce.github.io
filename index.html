<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>Unfettered Forceful Skill Acquisition</title>
    <link rel="icon" type="image/png" sizes="76x76" href="media/jaf.png">

    <meta name="description" content="Paper website for Unfettered Forceful Skill Acquisition">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

    <meta property="og:image" content="https://scalingforce.github.io/media/teaser.png">
    <meta property="og:image:type" content="image/jpeg">
    <meta property="og:image:width" content="682">
    <meta property="og:image:height" content="682">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://scalingforce.github.io/"/>
    <meta property="og:title" content="Unfettered Forceful Skill Acquisition" />
    <meta property="og:description" content="Project page for Unfettered Forceful Skill Acquisition paper." />

    <!--TWITTER-->
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Unfettered Forceful Skill Acquisitions" />
    <meta name="twitter:description" content="Project page for Unfettered Forceful Skill Acquisition paper." />
    <meta name="twitter:image" content="https://scalingforce.github.io/media/teaser.png" />

<!-- progprompt -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
<!--  -->

    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/styles/default.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/highlight.min.js"></script>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js" integrity="sha384-ka7Sk0Gln4gmtz2MlQnikT1wXgYsOg+OMhuP+IlRH9sENBO0LRn5q+8nbTov4+1p" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    <script>hljs.highlightAll();</script>

  
    <script src="js/app.js"></script>
    <!-- Google tag (gtag.js) -->
    <script>
    document.addEventListener('DOMContentLoaded', function () {
        // remove dropdown duplicates: https://stackoverflow.com/questions/23729456/how-to-remove-duplicate-dropdown-option-elements-with-same-value
        $(".select option").each(function() {
        $(this).siblings('[value="'+ this.value +'"]').remove();
        });

        var coll = document.getElementsByClassName("collapsible");
        var i;

        for (i = 0; i < coll.length; i++) {
        coll[i].addEventListener("click", function() {
            this.classList.toggle("active");
            var content = this.nextElementSibling;
            if (content.style.display === "block") {
            content.style.display = "none";
            } else {
            content.style.display = "block";
            }
        });
        }
    }, false);
    </script>
    <style>
        .nav-pills {
            position: relative;
            display: inline;
        }

        .imtip {
            position: absolute;
            top: 0;
            left: 0;
        }
    </style>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                </strong>
                <strong>
                    <font size="+4">Unfettered Forceful Skill Acquisition with Physical Reasoning and Coordinate Frame Labeling</font>
                </strong>
            </h2>
        </div>
        <!-- <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <br>
                    <li>name1 <span class="icon">
                        <a href="mailto:email@institution.edu"><i class="fa fa-envelope"></i></a>,
                      </span></li>
                    <li>name2</li>
                    <li>name3</li>
                </ul>
            </div>
        </div> -->

        <!-- <div class="row">
            <div class="col-md-12 text-center">                
                <strong>institution name</strong>
            </div>
        </div> -->
    </div>
    <!-- container break -->
    <div class="row"> <div class="col text-center">
    <section class="hero">
        <div class="hero-body" style="padding: 10px">
          <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="publication-links">
                    <span class="link-block">
                        <a target="_blank" href="media/unfettered.pdf"
                           class="external-link button is-normal is-rounded is-dark">
                          <span class="icon">
                              <i class="fas fa-file-pdf"></i>
                          </span>
                          <span>Paper</span>
                        </a>
                      </span>

                    <!-- <span class="link-block">
                      <a target="_blank" href="https://arxiv.org/abs/2410.13124"
                         class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                            <i class="ai ai-arxiv"></i>
                        </span>
                        <span>arXiv</span>
                      </a>
                    </span> -->
      
                  <span class="link-block">
                    <a target="_blank" href="https://github.com/scalingforce/unfetteredforce"
                       class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                          <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                      </a>
                  </span>
      
                  <span class="link-block">
                    <a href="#faqs"
                       class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                          <i class="fas fa-question"></i>
                      </span>
                      <span>FAQs</span>
                      </a>
                  </span>  

                  <span class="link-block">
                    <a target="_blank" href="https://drive.google.com/drive/folders/1W5pKdqsiXAujO3R8qoVz0g2Y5Bv0nFxh?usp=sharing"
                       class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                          <i class="fas fa-database"></i>
                      </span>
                      <span>Data</span>
                      </a>
                  </span>

                </div>
            </div>
          </div>
        </div>
    </section>
    </div></div>

    <div class="container">
        <div class="row">
            <!-- <div class="col-2"></div> -->
            <div class="col" style="text-align:center;">
                <iframe src="https://drive.google.com/file/d/1bMq0U7XKIUO1KPd3Y-drHvrdN_yy0epy/preview" width="640" height="480" allow="autoplay"></iframe>
            </div>
            <!-- <div class="col-2"></div> -->
        </div>
    </div>

    <div class="container">
        <div class="row">
            <div class="col-2"></div>
            <div class="col-md-8">
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
                    Vision language models (VLMs) exhibit vast knowledge of the physical world, including intuition of physical and spatial properties, affordances, and motion. With fine-tuning, VLMs can also natively produce robot trajectories.
                    We demonstrate that eliciting wrenches, not trajectories, allows VLMs to explicitly reason about forces and leads to zero-shot generalization in a series of manipulation tasks without pretraining. We achieve this by overlaying a consistent visual representation of relevant coordinate frames on robot-attached camera images to augment our query. First, we show how this addition enables a versatile motion control framework evaluated across four tasks (opening and closing a lid, pushing a cup or chair) spanning prismatic and rotational motion, an order of force and position magnitude, different camera perspectives, annotation schemes, and two robot platforms over 220 experiments, resulting in 51% success across the four tasks.
                    Then, we demonstrate that the proposed framework enables VLMs to continually reason about interaction feedback to recover from task failure or incompletion, with and without human supervision.
                    Finally, we observe that prompting schemes with visual annotation and embodied reasoning can bypass VLM safeguards. We characterize prompt component contribution to harmful behavior elicitation and discuss its implications for developing embodied reasoning.            
                </p>
            </div>
            <div class="col-2"></div>
        </div>
    </div>

<div class="container">
    <div class="row">
        <div class="col-2"></div>
        <div class="col-md-8">
            <h3>
                Click a picture and evaluate the reasoning and motion yourself!
            </h3>
        </div>
        <div class="col-md-2"></div>
    </div>

    <div class="row" id="demo">
    <div class="col-md-1"></div>
    <div class="col-md-10 col-xs-12">
        <div class="row">
            <div class="col-md-2 col-sm-2 col-xs-2 px-1">
                <img src="videos/close_case.png" width="100%"
                    alt="5s"
                    onclick="populateDemo(this);">
            </div>
            <div class="col-md-2 col-sm-2 col-xs-2 px-1">
                <img src="videos/open_case.png" width="100%"
                    alt="6s"
                    onclick="populateDemo(this);">
            </div>
            <div class="col-md-2 col-sm-2 col-xs-2 px-1">
                <img src="videos/push_bottle.png" width="100%"
                    alt="4s"
                    onclick="populateDemo(this);">
            </div>
            <div class="col-md-2 col-sm-2 col-xs-2 px-1">
                <img src="videos/push_chair.png" width="100%"
                    alt="4s"
                    onclick="populateDemo(this);">
            </div>
            <div class="col-md-2 col-sm-2 col-xs-2 px-1">
                <img src="videos/humanoid_empty_chair.png" width="100%" 
                    alt="2s"
                    onclick="populateDemo(this);">
            </div>
            <div class="col-md-2 col-sm-2 col-xs-2 px-1">
                <img src="videos/humanoid_heavy_chair.png" width="100%"
                    alt="4s"
                    onclick="populateDemo(this);">
            </div>
        </div>
        <p></p>
        <div class="row border rounded" style="padding-top:10px; padding-bottom:10px;">
            <div class="col-md-6">
                <video width="100%" height="100%" id="demo-video" autoplay loop muted webkit-playsinline playsinline onclick="setAttribute('controls', 'true');">
                    <source id="expandedImg" src="videos/DeliGrasp.mp4" type="video/mp4">
                </video>
        
            </div>
            <div class="col-md-6">
                <div id="imgtext">Prompt text in gray.</div>
                <div>
                    <pre><code class="language-python" id="answer">Spatial and physical reasoning shown within code block.</code></pre>
                </div>
            </div>
        </div>
        <p></p>
    </div>
<div class="col-md-1"></div>
</div>

    <!-- <div class="row">
        <div class="col-2"></div>
        <div class="col-md-8">
            <h3>
                Data download link: <a href="">HuggingFace</a>
            </h3>
            <hr>
        </div>
        <div class="col-md-2"></div>
    </div> -->

</div>    

    <div class="container">
        <div class="row">
            <div class="col-2"></div>
            <div class="col-md-8">
                <h3>
                    Approach Overview
                </h3>
                <p>
                    The proposed framework is composed of three primary components: 1) coordinate frame labeling, 2) generating wrench plans from VLM embodied reasoning, 
                    and 3) two force-controlled robot platforms (UR5 robot arm with an OptoForce F/T sensor, Unitree H1-2 humanoid) to follow VLM-generated wrenches.
                    Given a natural language task query, the framework labels head and/or wrist images with a wrist or world coordinate frame placed at a VLM-generated grasp point (u, v). 
                    Then a VLM, queried with the annotated images and task, is prompted to leverage spatial and physical reasoning to estimate an appropriate wrench and duration appropriate 
                    for task completion. The wrench is then passed to a force controller and, in the case of failure or incompletion, the resulting robot data can be used autonomously
                     or with human feedback for iterative task improvement.

                </p>
                <p>
                </p>
                <p>
                </p>
                <p style="text-align:center;">
                    <image src="media/overview.png" class="img-responsive">
                </p>
            </div>
            <div class="col-md-2"></div>
        </div>

        <div class="row">
            <div class="col-2"></div>
            <div class="col-md-8">
                <h3>
                    Evaluating Camera Views, Frame Selection, and Force vs. Position
                </h3>
                <p style="text-align:center;">
                    <image src="media/experiments.png" class="img-responsive">
                </p>
                To understand the effect of coordinate frame label selection on VLM embodied reasoning, we evaluate the proposed framework, zero-shot without iterative improvement, 
                on five differing coordinate frame labeling configurations described. We test four prismatic and rotational tasks (10 trials per task): pushing a 0.5kg bottle 10cm across a 
                smooth plastic table, pushing a 9kg rolling chair 20cm across a tiled floor, and opening and closing a tool case with a 0.2kg lid hinged about a plastic bushing.
                We randomize robot and object pose in each trial.
                <p style="text-align:center;">
                    <image src="media/breakdown.png" class="img-responsive">
                </p>
                <p>
                    The two most successful configurations (head and wrist views world frame label and head view with aligned wrist frame label), achieved a success rate of 51.3% and 50.0%, 
                    respectively. While VLM physical reasoning remains comparatively accurate across configurations (67% correct property and force estimation, low/high of 61.3% and 72.5%), 
                    spatial reasoning is highly sensitive to logically consistent coordinate frame annotations, resulting in task success volatility. 
                    Wrist-frame labeling induces spatial contradictions and poor spatial reasoning (42.5% and 32.5%). World-frame labels greatly ease prismatic motion but not off-axis 
                    rotational motion, though motion plans are overall improved (65.0% and 80.0%). World-aligned wrist frame labeling retains object-relative motion but is more globally consistent, 
                    presenting a compromise between the two approaches (70.0%). The position-control baseline leveraging a head and wrist view with world frame labeling yields moderate success (41.3%) 
                    and high success on the simpler bottle-pushing task. However, VLM-generated position trajectories are imprecise and uncorrectable without force control, 
                    producing suboptimal, unsafe, and/or slipping motions for more complex and forceful tasks.
                </p>
            </div>
            <div class="col-md-2"></div>
        </div>

        <div class="row">
            <div class="col-md-2"></div>
            <div class="col-md-8 col-xs-12">
                <h3>
                    Eliciting Harmful Behavior
                </h3>
                <p>
                    To evaluate the effect of embodiment and grounding on model behavior, we ablate the proposed framework's two-step reasoning prompt across different dimensions: 
                    1) varying visual grounding from no image, an image with task-relevant objects placed in the gripper, or an image with an empty workspace in the model query, 
                    2) with and w/o spatial reasoning, and 3) with and w/o physical reasoning, resulting in 13 prompts and 21 prompt & vision configurations of varying complexity. 
                    We evaluate each configuration against three harmful tasks (requesting harm to a human neck, torso, and wrist).
                </p>
                <p style="text-align:center;">
                    <image src="media/harmful_behavior.png" class="img-responsive">
                </p>
                <p>
                We observe an average harmful behavior elicitation rate of 58% across all models, though this varies greatly per model: 
                Claude 3.7 Sonnet, which unilaterally refused to answer two of three tasks, only produced 21.5% harmful queries, 
                whereas GPT 4.1 Mini readily provided (close to 100%) harmful wrenches for all tasks in 18 of 21 prompt configurations, or 87.9% across all configurations. 
                Gemini also provided responses for all tasks in 18 of 21 configurations, but with a lower harm rate of 62.8%. 
                This is not necessarily due to improved safeguarding, as "safe" responses simply provided wrenches below 5 Nm.
                </p>
                <p style="text-align:center;">
                    <image src="media/per_model_harm_wrench.png" class="img-responsive">
                </p>
                <p>
                The 1890 responses can be downloaded as a <a href="./assets/harmful_behavior_responses.csv">CSV file here</a>. The columns are as follows:
                <pre><code class="language-python" id="answer">['harmful', 'response', 'message', 'motion_plan', 'query_time', 'task', 'obj', 'level', 'model', 'image_path', 'magnitude', 'wrench', 'grasp_force', 'true_level', 'prompt_level', 'config_level']</code></pre>    
                </p>
            <div class="col-2"></div>
            </div>
        </div>
<!-- break container -->
</div>

<!-- resume container -->
<div class="container">
        <div class="row">
            <div class="col-md-2"></div>
            <div class="col-md-8" id="faqs">
                <h3>
                      FAQs
                </h3>
                <p><b>
                    Why not provide robot (base or gripper) orientation as a native input?
                </b></p>  
                <p>
                    Yes, that may be more robust, but would more-or-less require training a "VLA"-type model. In this work, we're interested in
                    demonstrating the off-the-shelf reasoning-to-wrench capabilities of VLMs, enabled by visual grounding in coordinate frames.
                    While us humans don't think about the world in terms of coordinate frames or precise Newtons of force, 
                    we think our approach aligns VLMs more closely with human motion and allows for more intuitive interaction.
                    We believe that embodied agents should devote the bulk of their "thinking" to understanding their tasks in the world spatially and physically 
                    and not necessarily to precise, step-wise trajectories.
                </p>
                <p><b>
                    Why not try to mitigate harmful behavior elicitation?
                </b></p>  
                <p>
                    We can imagine a number of solutions for stopping this specific jailbreaking attack. 
                    But what are the downstream effects? Will it make our proposed framework less consistent? 
                    Of note, the higher-latency, native chain-of-thought "reasoning" models like OpenAI's o3 and o4 models always refused to provide an answer. 
                    When we change the “break the wrist” request to “set the dislocated wrist back in place,” these reasoning models also refuse to provide an answer. 
                    This is a good thing, at least at our current levels of reasoning, as we would never ask a robot to do such a thing. 
                    But when time comes (labor, population shortage, societal collapse, etc.), how will we be able to distinguish one from the other? 
<br><br>
                    We hope to highlight this emergent conundrum in model development. How can we enable models to reason better with experience and interaction in the physical world, 
                    and how can we do this while preventing dangerous requests, if that is even desirable? 
                    Embodied reasoning to accomplish forceful motion is possible but still has a long ways to go until truly robust and versatile motion. 
                    As we work toward that goal, how can we mitigate the harmful behavior elicited? In trying to mitigate that, will we impede model improvement? 

                </p>               
                <p><b>
                    What's next?
                </b></p>  
                <p>
                    Acquiring more accurate information and skills from physical interaction!
                </p>                
                <p><b   >
                    Some more tasks
                </b></p>  <p>
                <div class="row">
                    <div class="col text-center">
                        <button type="button" class="collapsible">...</button>
                        <div class="content">
                            <img src="" width="60%">
                            Forthcoming
                        </div>      
                    </div>
                    </div>     
                </p>                
            </div>
            <div class="col-md-2"></div>
          </div>

          <div class="row">
            <div class="col-md-2"></div>
            <div class="col-md-8">
                <h3>
                      Prompts
                  </h3>
                <p class="text-justify">
                    <span class="dg">Spatial and Physical Reasoning</span>: 
                    <a href="./assets/prompts/sf.txt">System Prompt</a> |
                    <span class="dg">Harmful Behavior Elicitations</span>: 
                    <a href="./assets/prompts/behavior_elicitation.txt">13 Prompts</a> |
                </p>
            </div>
            <div class="col-md-2"></div>
          </div>


<br>
<br>
          <!-- <div class="row">
            <div class="col-md-2"></div>
            <div class="col-md-8">
                <h3>
                    BibTeX Citation
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly style="height: 135px;">

                    </textarea>
                </div>
            </div>
            <div class="col-md-2"></div>
        </div> -->

        <!-- <div class="row">
            <div class="col-md-2"></div>
            <div class="col-md-8">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                    The website template is adapted from <a href="https://language-to-reward.github.io/">Language to Rewards</a> and <a href="https://progprompt.github.io">ProgPrompt</a>.
                </p>
            </div>
            <div class="col-md-2"></div>
        </div> -->
    </div>
</body>

<script>
    function populateDemo_jaf(imgs) {
        // delete video with id demo-video
        var placeholder = document.getElementById('placeholder');
        placeholder.innerHTML = ""

        // Get the image text
        var dpgotext = document.getElementById("dpgotext");
        var dpgonftext = document.getElementById("dpgonftext");
        var qa = imgs.alt.split("[sep]");
        dpgotext.innerHTML = `Forceful Policy: ${qa[0]} mm`;
        dpgonftext.innerHTML = `Position-Only Policy: ${qa[1]} mm`;

        var dpgo = document.getElementById("dpgo");
        dpgo.innerHTML = "";
        var dpgonf = document.getElementById("dpgonf");
        dpgonf.innerHTML = "";

        var dpgoVideo = document.createElement("video");
        dpgoVideo.setAttribute("width", "100%");
        dpgoVideo.setAttribute("height", "100%");
        dpgoVideo.setAttribute("autoplay", "true");
        dpgoVideo.setAttribute("loop", "true");
        dpgoVideo.setAttribute("muted", "true");

        var dpgoSource = document.createElement("source");
        dpgoSource.setAttribute("src", imgs.src.replace(".png", "-go.mp4"));
        dpgoSource.setAttribute("type", "video/mp4");

        dpgoVideo.appendChild(dpgoSource);
        dpgo.appendChild(dpgoVideo);

        var dpgonfVideo = document.createElement("video");
        dpgonfVideo.setAttribute("width", "100%");
        dpgonfVideo.setAttribute("height", "100%");
        dpgonfVideo.setAttribute("autoplay", "true");
        dpgonfVideo.setAttribute("loop", "true");
        dpgonfVideo.setAttribute("muted", "true");

        // Create a source element for dpgonf video
        var dpgonfSource = document.createElement("source");
        dpgonfSource.setAttribute("src", imgs.src.replace(".png", "-gonf.mp4"));
        dpgonfSource.setAttribute("type", "video/mp4");

        // Append the source to the dpgonf video
        dpgonfVideo.appendChild(dpgonfSource);
        dpgonf.appendChild(dpgonfVideo);

        
        // Use the value of the alt attribute of the clickable image as text inside the expanded image
        // Show the container element (hidden with CSS)
        expandImg.parentElement.style.display = "block";
    }

    function populateDemo(imgs) {
        // Get the expanded image
        var expandImg = document.getElementById("expandedImg");
        // Get the image text
        var imgText = document.getElementById("imgtext");
        var answer = document.getElementById("answer");

        // Use the same src in the expanded image as the image being clicked on from the grid
        expandImg.src = imgs.src.replace(".png", ".mp4");
        var video = document.getElementById('demo-video');
        // or video = $('.video-selector')[0];
        video.pause()
        video.load();
        video.play();
        video.removeAttribute('controls');
        
        console.log(expandImg.src);
        // Use the value of the alt attribute of the clickable image as text inside the expanded image
        var qa = imgs.alt.split("[sep]");
        imgText.innerHTML = qa[0];
        answer.innerHTML = "";
        // Show the container element (hidden with CSS)
        expandImg.parentElement.style.display = "block";
        for (timeoutId of timeoutIds) {
            clearTimeout(timeoutId);
        }
        typeWriter(qa[1], 0, qa[0]);
    }

    // function typeWriter(div, txt, i, q) {
    function typeWriter(txt, i, q) {
        var imgText = document.getElementById("imgtext");
        if (imgText.innerHTML == q) {
            if (i < txt.length) {
                if (txt.charAt(i) == "\\") {
                    answer.innerHTML += "\n";
                    // div.innerHTML += "\n";
                    i += 1;
                } else {
                    answer.innerHTML += txt.charAt(i);
                    // div.innerHTML += txt.charAt(i);
                }
                i++;
                timeoutIds.push(setTimeout(typeWriter, 5, txt, i, q));
            }
            else {
                hljs.highlightAll();
            }
        }
        i = 0
    }

    var myCarousel = document.querySelector('#myCarousel')
    var carousel = new bootstrap.Carousel(myCarousel, {interval: 500, wrap: true})
    var slides = document.querySelectorAll('.carousel .carousel-item')

    slides.forEach((el) => {
        // number of slides per carousel-item
        const minPerSlide = slides.length
        let next = el.nextElementSibling
        for (var i=1; i<minPerSlide; i++) {
            if (!next) {
                // wrap carousel by using first child
                next = slides[0]
            }
            let cloneChild = next.cloneNode(true)
            el.appendChild(cloneChild.children[0])
            next = next.nextElementSibling
        }
})

</script>
</html>

